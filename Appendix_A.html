<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta name=viewport content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .indented {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            font-weight: normal;
            padding-left: 20px;
            margin-bottom: -40px;
            margin-top: -0px;
        }

        .first-author-paper {
            display: table-row;
        }

        .collaborative-paper {
            display: none;
        }

        span.highlight {
            background-color: #ffffd0;
        }

        button {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            background-color: #ffffff;
            /* 修改背景为白色 */
            border: 1px solid #ddd;
            /* 添加浅灰色边框 */
            color: #888;
            /* 修改文本为灰色 */
            padding: 10px 15px;
            font-size: 14px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.2s, transform 0.2s;
            display: inline-block;
        }

        button:hover {
            background-color: #f2f2f2;
            /* 鼠标悬停时的背景颜色 */
            transform: scale(1.05);
        }

        button:focus {
            outline: none;
        }

        button.active {
            background-color: #d3d3d3;
            /* Gray color for active button */
        }

        #first-author-btn,
        {
            width: 180px;
            height: 40px;
        }

        #collaborative-btn {
            width: 120px;
            height: 40px;
        }

        .up-one-line {
            margin-top: 0em;
        }

        .papers-heading {
            margin-bottom: 10px;
            /* Adjust as needed */
        }

        .papers-table {
            margin-top: -90px;
            /* Adjust as needed */
        }
    </style>

    <link rel="icon" type="image/jpg" href="images/seal_icon.jpg">
    <title>Yan Ding</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<!-- Add JavaScript to toggle visibility -->
<script type="text/javascript">
    function togglePapers(category) {
        var firstAuthorPapers = document.getElementsByClassName('first-author-paper');
        var firstAuthorBtn = document.getElementById('first-author-btn');
        var collaborativePapers = document.getElementsByClassName('collaborative-paper');
        var collaborativeBtn = document.getElementById('collaborative-btn');
        var preprintPapers = document.getElementsByClassName('preprint-paper');
        var preprintBtn = document.getElementById('preprint-btn');

        if (category === 'first-author') {
            for (var i = 0; i < firstAuthorPapers.length; i++) {
                firstAuthorPapers[i].style.display = 'table-row';
            }
            for (var i = 0; i < collaborativePapers.length; i++) {
                collaborativePapers[i].style.display = 'none';
            }
            for (var i = 0; i < preprintPapers.length; i++) {
                preprintPapers[i].style.display = 'none';
            }
            firstAuthorBtn.style.backgroundColor = '#f2f2f2';
            collaborativeBtn.style.backgroundColor = '#ffffff';
            preprintBtn.style.backgroundColor = '#ffffff';
        }
        if (category === 'collaborative') {
            for (var i = 0; i < firstAuthorPapers.length; i++) {
                firstAuthorPapers[i].style.display = 'none';
            }
            for (var i = 0; i < collaborativePapers.length; i++) {
                collaborativePapers[i].style.display = 'table-row';
            }
            for (var i = 0; i < preprintPapers.length; i++) {
                preprintPapers[i].style.display = 'none';
            }
            firstAuthorBtn.style.backgroundColor = '#ffffff';
            collaborativeBtn.style.backgroundColor = '#f2f2f2';
            preprintBtn.style.backgroundColor = '#ffffff';
        }
        if (category === 'preprint') {
            for (var i = 0; i < firstAuthorPapers.length; i++) {
                firstAuthorPapers[i].style.display = 'none';
            }
            for (var i = 0; i < collaborativePapers.length; i++) {
                collaborativePapers[i].style.display = 'none';
            }
            for (var i = 0; i < preprintPapers.length; i++) {
                preprintPapers[i].style.display = 'table-row';
            }
            firstAuthorBtn.style.backgroundColor = '#ffffff';
            collaborativeBtn.style.backgroundColor = '#ffffff';
            preprintBtn.style.backgroundColor = '#f2f2f2';
        }
    }
</script>

<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <p align="center">
                                <name>Yan Ding</name>
                            </p>
                            <p>I am a Researcher at Shanghai AI Laboratory, working on AI-driven embodied agents and decision-making systems, directed by Professor <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en&oi=ao">Xuelong Li</a>. I completed my PhD in Computer Science at the State University of New York (SUNY), Binghamton, in February 2024. I supported by grants from the Ford Motor Company. I also got the Academic Excellence in Computer Science (PhD) at Binghamton University. I was supervised by Professor <a href="http://www.cs.cqu.edu.cn/info/1322/6092.htm">Chao Chen</a>, during my master's program. I received my M.S. in Computer Science in 2019, and got my B.S. in Mechanical Engineering in 2016 from Chongqing University, China. The master's thesis, awarded the Outstanding Thesis of Chongqing City, is available via this <a href="./project/PDF/dissertation_master.pdf">Link</a>.
                            </p>
                            <p>
                                [<a href="http://124.223.30.221:81/"> Homepage (No VPN Needed)</a>] [<a href="https://scholar.google.com/citations?user=0rP_rGUAAAAJ&hl=en">Google Scholar</a>] [<a href="./project/PDF/CV.pdf">CV (Dec 2023)</a>]
                            </p>

                            <p><span style="color: red;">I am seeking interns for Embodied AI and Robotics!</span> Feel free to contact me at yding25@binghamton.edu.
                            </p>
                        </td>
                        <td width="33%">
                            <img src="img/photo.jpg" width="160" height="225">
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <heading>Research Direction</heading>
                            <p>The current research interests include:</p>
                            <ul>
                                <li><span style="color: #1772d0;">Spatial Intelligence for Robotics</span>: Empowering Robots to Understand the Real World
                                </li>
                                <li><span style="color: #1772d0;">Skill Learning for Robotics</span>: Enabling Robots to Transform the Real World
                                </li>
                            </ul>
                            <p>with a particular emphasis on their applications in the context of <span style="color: #1772d0;">mobile manipulators (MoMa)</span>.</p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <heading>Robotic Tool</heading>
                            <p>My team has several robots, including five robotic arms and two mobile manipulators. Therefore, we are developing an open-source robotic tool called <a href="http://bestmanrobot.com/">BestMan</a>. This tool supports development both in simulation and on real machines. By using a unified framework, BestMan facilitates rapid development, helping researchers save significant time. (Note: BestMan is still under construction.)
                            </p>
                            <p>This project encompasses various sub-projects (selected):
                            <ul>
                                <li>
                                    <a href="https://github.com/AutonoBot-Lab/BestMan_Pybullet">BestMan_Pybullet</a>
                                    <img src="https://img.shields.io/github/stars/AutonoBot-Lab/BestMan_Pybullet?style=social" alt="GitHub stars">
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Flexiv">BestMan_Flexiv (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Flexiv?style=social" alt="GitHub stars"> -->
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Xarm">BestMan_Xarm (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Xarm?style=social" alt="GitHub stars"> -->
                                </li>
                                <!-- 
                                    <li>
                                        <a href="https://github.com/placeholder/BestMan_Franka">BestMan_Franka</a>
                                        <img src="https://img.shields.io/github/stars/placeholder/BestMan_Franka?style=social" alt="GitHub stars">
                                    </li>
                                    <li>
                                        <a href="https://github.com/placeholder/BestMan_Xarm">BestMan_Xarm</a>
                                        <img src="https://img.shields.io/github/stars/placeholder/BestMan_Xarm?style=social" alt="GitHub stars">
                                    </li>
                                    <li>
                                        <a href="https://github.com/placeholder/BestMan_RealMan">BestMan_RealMan</a>
                                        <img src="https://img.shields.io/github/stars/placeholder/BestMan_RealMan?style=social" alt="GitHub stars">
                                    </li>
                                    <li>
                                        <a href="https://github.com/placeholder/BestMan_RealMan_RangeMini">BestMan_RealMan_RangeMini</a>
                                        <img src="https://img.shields.io/github/stars/placeholder/BestMan_RealMan_RangeMini?style=social" alt="GitHub stars">
                                    </li>
                                    <li>
                                        <a href="https://github.com/placeholder/BestMan_Xarm_RangeMini">BestMan_Xarm_RangeMini</a>
                                        <img src="https://img.shields.io/github/stars/placeholder/BestMan_Xarm_RangeMini?style=social" alt="GitHub stars">
                                    </li>
                                    -->
                            </ul>
                            </p>
                            <p>This is my YouTube channel <a href="https://www.youtube.com/@yanding1760">&#64;yanding1760</a>, featuring several videos centered around robots.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                        <tr>
                            <td width="100%" valign="middle" class="papers-heading">
                                <h2>Interns / Graduate Students</h2>
                                <ul>
                                    <li><strong>Zhaxizhuoma</strong>&nbsp; (Internship Period: 2024.05 -- )  <br>Master, Technische Universität Berlin</li>
                                    <br>
                                    <li><strong><a
                                        href="https://ziniuw.com">Ziniu Wu</a></strong> &nbsp; (Internship Period: 2024.06 -- ) <br>PhD student, University of Bristol</li>
                                    <br>
                                    <li><strong>Yuhan Wu</strong> &nbsp; (Internship Period: 2024.07 -- ) <br>Master student, University of Chinese Academy of Sciences</li>
                                    <br>
                                    <li><strong>Chuyue Guan</strong> &nbsp; (Internship Period: 2024.08 -- ) <br>Master Student, Stanford University</li>
                                    <br>
                                    <li><strong>Tianyu Wang</strong> &nbsp; (Internship Period: 2024.08 -- ) <br>Master Student, Fudan University</li>
                                    <br>
                                    <li><strong>Zhongjie Jia</strong> <br> PhD Student, Shanghai Jiaotong University</li>
                                    <br>
                                    <li><strong>Kehui Liu</strong> <br> PhD Student, Northwestern Polytechnical University</li>
                                    <br>
                                    <li><strong>Pengyuan Wu</strong> <br> Pre-PhD Student, Zhejiang University</li>
                                </ul>
                            </td>
                        </tr>
                    </table>
                                                                            
                    
                    <table width="100%" align="center" border="0"
                        cellspacing="0" cellpadding="10">
                        <td width="100%" valign="middle" class="papers-heading">
                            <heading>Publication</heading>
                        </td>
                        <td width="100%" valign="middle" class="papers-heading">
                            <button id="first-author-btn"
                                onclick="togglePapers('first-author')">First(*)/Corresponding(#)</button>
                        </td>
                        <td width="100%" valign="middle" class="papers-heading">
                            <button id="collaborative-btn"
                                onclick="togglePapers('collaborative')">Collaborative</button>
                        </td>
                        <td width="100%" valign="middle" class="papers-heading">
                            <button id="preprint-btn"
                                onclick="togglePapers('preprint')">Preprint</button>
                        </td>
                    </table>

                    <table width="100%" align="center" border="0"
                        cellspacing="0" cellpadding="10" class="papers-table">

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/dkprompt.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning</papertitle>
                                </a>
                                <br>
                                Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, <strong>Yan Ding</strong>, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang
                                <br>
                                <em>Under Review</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2406.17659">Paper</a>]
                                <br>
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/mtech.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</papertitle>
                                </a>
                                <br>
                                Zhigen Zhao, Shuo Chen, <strong>Yan Ding</strong>, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao
                                <br>
                                <em>IEEE/ASME Transactions on Mechatronics, 2024</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2404.02817">Paper</a>]
                                <br>
                            </td>
                        </tr>

                        <tr class="preprint-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/moma_pos.jpg'
                                            width="160" vspace="5"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task Execution?</papertitle>
                                </a>
                                <br>
                                Beichen Shao*, <strong>Yan Ding*#</strong>, Xingchen Wang, Xuefeng Xie, Fuqiang Gu, Jun Luo, Chao Chen#
                                <br>
                                <em>Under Review</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2403.19940.pdf">Paper</a>]
                                [<a
                                    href="https://yding25.com/MoMa-Pos/">Project</a>]
                                [<a
                                    href="https://yding25.com/MoMa-Pos/static/assets/videos/demos/demo1.mp4">Video</a>]
                                [<a
                                    href="https://github.com/AutonoBot-Lab/Project_MoMa-Pos">Code</a>]
                                <br>
                                <p></p>
                                <p> Mobile manipulators always need to determine
                                    feasible base positions prior to carrying
                                    out navigation-manipulation tasks.
                                    Real-world environments are often cluttered
                                    with various furniture, obstacles, and
                                    dozens of other objects. Efficiently
                                    computing base positions poses a challenge.
                                    In this work, we introduce a framework named
                                    MoMa-Pos to address this issue.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/llm-grop.jpg'
                                            width="160" vspace="5"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Task and Motion Planning with
                                        Large Language Models for Object
                                        Rearrangement</papertitle>
                                </a>
                                <br>
                                <strong>Yan Ding</strong>, Xiaohan Zhang,
                                Chris Paxton, Shiqi Zhang
                                <br>
                                <em>International Conference on Intelligent
                                    Robots and Systems (IROS), 2023</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2303.06247.pdf">Paper</a>]
                                [<a
                                    href="https://sites.google.com/view/llm-grop">Project</a>]
                                [<a
                                    href="https://youtu.be/8q5WPkABfNE">Video</a>]
                                [<a
                                    href="https://colab.research.google.com/drive/1cSqoSc6Gk9KM9p-GwHSIIL5VfZICGW3B?usp=sharing">Code</a>]
                                <br>
                                <p></p>
                                <p>LLM-GROP is a method that uses prompting to
                                    extract commonsense knowledge about object
                                    configurations from a large language model
                                    and instantiates them with a task and motion
                                    planner, allowing for successful and
                                    efficient multi-object rearrangement in
                                    various environments using a mobile
                                    manipulator.</p>
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/Chelsea.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>ARDIE: AR, Dialogue, and Eye
                                        Gaze Policies for Human-Robot
                                        Collaboration</papertitle>
                                </a>
                                <br>
                                Chelsea Zou, Kishan Chandan, <strong>Yan
                                    Ding</strong>, Shiqi Zhang
                                <br>
                                <em>ICRA Workshop on CoPerception: Collaborative
                                    Perception and Learning, 2023</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2305.04685.pdf">Paper</a>]
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/s30.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Symbolic State Space
                                        Optimization for Long Horizon Mobile
                                        Manipulation Planning</papertitle>
                                </a>
                                <br>
                                Xiaohan Zhang, Yifeng Zhu, <strong>Yan
                                    Ding</strong>, Yuqian Jiang, Yuke Zhu, Peter
                                Stone, and Shiqi Zhang
                                <br>
                                <em>International Conference on Intelligent
                                    Robots and Systems (IROS), 2023</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2307.11889.pdf">Paper</a>]
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/UAI.jpg'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Learning to Reason about
                                        Contextual Knowledge for Planning under
                                        Uncertainty</papertitle>
                                </a>
                                <br>
                                Cheng Cui, Saeid Amiri, <strong>Yan
                                    Ding</strong>, Xingyue Zhan, Shiqi Zhang
                                <br>
                                <em>The Conference on Uncertainty in Artificial
                                    Intelligence (UAI), 2023</em>
                                <br>
                                [<a
                                    href="https://proceedings.mlr.press/v216/cui23a/cui23a.pdf">Paper</a>]
                            </td>
                        </tr>

                        <tr class="preprint-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/ORLA.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>ORLA*: Mobile Manipulator-Based
                                        Object Rearrangement with Lazy
                                        A</papertitle>
                                </a>
                                <br>
                                Kai Gao, <strong>Yan Ding</strong>, Shiqi Zhang,
                                Jingjin Yu
                                <br>
                                <em>Under Review</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2309.13707.pdf">Paper</a>]
                                <br>
                                <p></p>
                                <p>In this research, we propose ORLA*, which
                                    leverages delayed (lazy) evaluation in
                                    searching for a high-quality object pick and
                                    place sequence that considers both
                                    end-effector and mobile robot base
                                    travel.</p>
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/tpvqa.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <br>

                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Grounding Classical Task
                                        Planners via Vision-Language
                                        Models</papertitle>
                                </a>
                                <br>
                                Xiaohan Zhang, <strong>Yan Ding</strong>, Saeid
                                Amiri, Hao Yang, Andy Kaminski, Chad Esselink,
                                and Shiqi Zhang
                                <br>
                                <em>ICRA Workshop on Robot Execution Failures
                                    and Failure Management Strategies, 2023</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2304.08587.pdf">Paper</a>]
                                <br>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img
                                            src='img/cowp_real.png' width="160"
                                            vspace="50"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Integrating Action Knowledge and
                                        LLMs for Task Planning and Situation
                                        Handling in Open Worlds</papertitle>
                                </a>
                                <br>
                                <strong>Yan Ding</strong>, Xiaohan Zhang, Saeid
                                Amiri, Nieqing Cao, Hao Yang, Chad Esselink,
                                Shiqi Zhang
                                <br>
                                <em>Autonomous Robots (accepted)</em>
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2305.17590.pdf">Paper</a>]
                                [<a
                                    href="https://cowplanning.github.io/">Project</a>]
                                [<a
                                    href="https://youtu.be/HtxlXSzY5VQ">Video</a>]
                                [<a
                                    href="https://github.com/yding25/GPT-Planner">Code</a>]
                                <br>
                                <p></p>
                                <p>The paper introduces a new algorithm (COWP)
                                    that uses task-oriented common sense
                                    extracted from Large Language Models to help
                                    robots handle unforeseen situations and
                                    complete complex tasks in an open world,
                                    with better success rates than previous
                                    algorithms.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/tmoc.png'
                                            width="160" vspace="30"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Learning to Ground Objects for
                                        Robot Task and Motion
                                        Planning</papertitle>
                                </a>
                                <br>
                                <strong>Yan Ding</strong>, Xiaohan Zhang,
                                Xingyue Zhan, Shiqi Zhang
                                <br>
                                <em>IEEE Robotics and Automation Letters
                                    (RA-L)</em>, 2022
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2202.06674.pdf">Paper</a>]
                                [<a
                                    href="https://yding25.github.io/project/TMOC/TMOC.html">Project</a>]
                                [<a
                                    href="https://github.com/yding25/TMOC">Code</a>]
                                [<a
                                    href="https://www.youtube.com/embed/3ijtbbeCQho">Presentation</a>]
                                <br>
                                <p></p>
                                <p>The paper presents a new robot planning
                                    algorithm, TMOC, which can handle complex
                                    real-world scenarios without prior knowledge
                                    of object properties by learning them
                                    through a physics engine, outperforming
                                    existing algorithms.</p>
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img
                                            src='img/grop_real.png' width="160"
                                            vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Visually Grounded Task and
                                        Motion Planning for Mobile
                                        Manipulation</papertitle>
                                </a>
                                <br>
                                Xiaohan Zhang, Yifeng Zhu, <strong>Yan
                                    Ding</strong>, Yuke Zhu, Peter Stone, and
                                Shiqi Zhang
                                <br>
                                <em>International Conference on Robotics and
                                    Automation (ICRA)</em>, 2022
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2202.10667.pdf">Paper</a>]
                                [<a
                                    href="https://sites.google.com/view/grop-tamp">Project</a>]
                            </td>
                        </tr>

                        <tr class="collaborative-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/howie.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Task and Situation Structures
                                        for Case-Based Planning</papertitle>
                                </a>
                                <br>
                                Hao Yang, Tavan Eftekhar, Chad Esselink,
                                <strong>Yan Ding</strong>, Shiqi Zhang
                                <br>
                                <em>International Conference on Case-Based
                                    Reasoning (ICCBR)</em>, 2021
                                <br>
                                [<a
                                    href="https://link.springer.com/chapter/10.1007/978-3-030-86957-1_18">Paper</a>]
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/tmpud.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Task-Motion Planning for Safe
                                        and Efficient Urban Driving</papertitle>
                                </a>
                                <br>
                                <strong>Yan Ding</strong>, Xiaohan Zhang,
                                Xingyue Zhan, Shiqi Zhang
                                <br>
                                <em>International Conference on Intelligent
                                    Robots and Systems (IROS)</em>, 2020.
                                <br>
                                [<a
                                    href="https://arxiv.org/pdf/2003.03807">Paper</a>]
                                [<a
                                    href="https://yding25.com/TMPUD/website/TMPUD.html">Project</a>]
                                [<a
                                    href="https://github.com/yding25/TMPUD">Code</a>]
                                [<a
                                    href="https://www.youtube.com/watch?v=8NHQYUqMyoI">Demo</a>]
                                [<a
                                    href="https://youtu.be/k-Pcnx8zgxE">Presentation</a>]
                                <br>
                                <p></p>
                                <p>Autonomous vehicles need to balance
                                    efficiency and safety when planning tasks
                                    and motions, and the algorithm Task-Motion
                                    Planning for Urban Driving (TMPUD) enables
                                    communication between planners for optimal
                                    performance.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/DVAT.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>DAVT: an error-bounded vehicle
                                        trajectory data representation and
                                        compression framework</papertitle>
                                </a>
                                <br>
                                Chao Chen*, <strong>Yan Ding*</strong>, Suiming
                                Guo, Yasha Wang
                                <br>
                                <em>IEEE TVT</em>, 2020.
                                <br>
                                [<a
                                    href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163296&casa_token=ZbgAMop0EiYAAAAA:BVnLHMxb3jaLs0Ukmq_RTszRARtuiPh5qg51GXNgxmmOsr5GbGna31OvxG6WnB8gfnyEJQUb">PDF</a>]
                                <br>
                                <p></p>
                                <p>DAVT proposes a mobile edge computing
                                    solution for vehicle trajectory data
                                    compression, which reduces data at the
                                    source and lowers communication and storage
                                    costs, using three compressors for distance,
                                    acceleration, velocity, and time data parts,
                                    and outperforms other baselines according to
                                    evaluation results.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/VTracer.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>VTracer: When online vehicle
                                        trajectory compression meets mobile edge
                                        computing</papertitle>
                                </a>
                                <br>
                                Chao Chen*, <strong>Yan Ding*</strong>, Zhu
                                Wang, Junfeng Zhao, Bin Guo, Daqing Zhang
                                <br>
                                <em>IEEE Systems Journal</em>, 2019.
                                <br>
                                [<a
                                    href="https://hal.science/hal-02321015/document">PDF</a>]
                                <br>
                                <p></p>
                                <p>This paper proposes an online trajectory
                                    compression framework that uses SD-Matching
                                    for GPS alignment and HCC for compression,
                                    and demonstrates its effectiveness and
                                    efficiency using real-world datasets in
                                    Beijing and deployment in Chongqing.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img
                                            src='img/trajcompressor.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>TrajCompressor: An Online
                                        Map-matching-based Trajectory
                                        Compression Framework Leveraging Vehicle
                                        Heading Direction and
                                        Change</papertitle>
                                </a>
                                <br>
                                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                                Xie, Shu Zhang, Zhu Wang, Liang Feng
                                <br>
                                <em>IEEE TITS</em>, 2019.
                                <br>
                                [<a
                                    href="https://ieeexplore.ieee.org/abstract/document/8697124">PDF</a>]
                                <br>
                                <p></p>
                                <p>This paper presents an online trajectory
                                    compression framework for reducing storage,
                                    communication, and computation issues caused
                                    by massive and redundant vehicle trajectory
                                    data, consisting of two phases: online
                                    trajectory mapping and trajectory
                                    compression, using Spatial-Directional
                                    Matching and Heading Change Compression
                                    algorithms respectively, which have been
                                    evaluated with real-world datasets in
                                    Beijing and deployed in Chongqing, showing
                                    higher accuracy and efficiency compared to
                                    state-of-the-art algorithms.
                                </p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img src='img/api.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Fuel Consumption Estimation of
                                        Potential Driving Paths by Leveraging
                                        Online Route APIs</papertitle>
                                </a>
                                <br>
                                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                                Xie, Xuefeng Xie, Zhikai Yang
                                <br>
                                <em>Green, Pervasive, and Cloud Computing: 13th
                                    International Conference (GPC)</em>, 2018.
                                <br>
                                [<a
                                    href="https://link.springer.com/chapter/10.1007/978-3-030-15093-8_7">PDF</a>]
                                <br>
                                <p></p>
                                <p>This paper proposes a fuel consumption model
                                    based on GPS trajectory and OBD-II data,
                                    which can estimate the fuel usage of driving
                                    paths and help drivers choose fuel-efficient
                                    routes to reduce greenhouse gas and
                                    pollutant emissions.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img
                                            src='img/threestage.png' width="160"
                                            vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>A three-stage online
                                        map-matching algorithm by fully using
                                        vehicle heading direction</papertitle>
                                </a>
                                <br>
                                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                                Xie, Shu Zhang
                                <br>
                                <em>Journal of Ambient Intelligence and
                                    Humanized Computing</em>, 2018.
                                <br>
                                [<a
                                    href="https://link.springer.com/article/10.1007/s12652-018-0760-0">PDF</a>]
                                <br>
                                <p></p>
                                <p>The SD-Matching algorithm proposes a
                                    three-stage approach to improve the accuracy
                                    and speed of online map-matching by
                                    incorporating vehicle heading direction
                                    data.</p>
                            </td>
                        </tr>

                        <tr class="first-author-paper">
                            <td width="25%">
                                <div class="one">
                                    <div class="two"><img
                                            src='img/greenplanner.png'
                                            width="160" vspace="20"></div>
                                </div>
                            </td>
                            <td valign="middle" width="75%">
                                <a>
                                    <papertitle>Greenplanner: Planning
                                        personalized fuel-efficient driving
                                        routes using multi-sourced urban
                                        data</papertitle>
                                </a>
                                <br>
                                <strong>Yan Ding*</strong>, Chao Chen*, Shu
                                Zhang, Bin Guo, Zhiwen Yu, Yasha Wang
                                <br>
                                <em>IEEE PerCom</em>, 2017.
                                <br>
                                [<a
                                    href="https://www.researchgate.net/profile/Chao-Chen-82/publication/311588334_GreenPlanner_Planning_Personalized_Fuel-efficient_Driving_Routes_using_Multi-sourced_Urban_Data/links/5a67db75a6fdcce9c106ed92/GreenPlanner-Planning-Personalized-Fuel-efficient-Driving-Routes-using-Multi-sourced-Urban-Data.pdf">PDF</a>]
                                <br>
                                <p></p>
                                <p>Greenhouse gas emissions from vehicles in
                                    modern cities is a significant problem, but
                                    recommending fuel-efficient routes to
                                    drivers through a personalized fuel
                                    consumption model can help alleviate this
                                    issue, as demonstrated by the successful
                                    implementation of GreenPlanner in Beijing,
                                    which achieved a mean fuel consumption error
                                    of less than 7% and an average savings of
                                    20% fuel consumption for suggested
                                    routes.</p>
                            </td>
                        </tr>

                        <script
                            src="https://cdn.jsdelivr.net/gh/yasserelsaid/chatbot@latest/index.min.js"
                            id="yding25-com-8yejwvr6d"></script>
                    </table>
                </td>
            </tr>
        </table>

        <table width="100%" cellspacing="0" cellpadding="20" border="0"
            align="center">
            <tbody>
                <tr>
                    <td>
                        <br>
                        <p align="center">
                            <font size="2">
                                Template from <a
                                    href="https://github.com/jonbarron/jonbarron_website">here.</a>
                            </font>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>

    </body>
